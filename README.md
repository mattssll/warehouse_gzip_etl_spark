### Project Structure:
./airflow_dag/ - this folder contains the DAG that will be ran by airflow and also a couple of scripts to raise airflow webserver and scheduler. <br>
It also creates an user with login=admin and password=admin.  <br>
./app.py - is responsible for launching the airflow script and starting airflow. To start the app (after installing the dependencies in an virtual environment) simply cd in the terminal into the project root folder and write "python app.py" in your terminal.  <br>
./app_spark - this is where most of my implementation is, inside the method folder you can see the methods that I called multiple times in process_metadata.py and process_reviews.py - these two .py files are sent through a spark-submit with spark. <br>
./app_viz_report - pandas_charts.py is inside this folder, this .py file is responsible for querying postgresql when the ETL is done. It build some visualizations with matplotlib, saves it to a .pdf and send it to email to a list of users defined in our dag.  <br>
./input_data/ - airflow downloads both json.gzips to this folder, they're then split by a bash command with airflow
./output_data/ - holds the cleaned data from the etl process in the format of fact and dimensions, this is generated by the airflow job with spark <br>
./output_report/ - this folder receives a .pdf with some visualizations that python did for us, querying postgresql. This is sent by email too.<br> List of users that receive this can be added in operator in the dag <br>
./scripts_psql - scripts responsible for creating the database in postgresql, the tables, and also responsible scripts that do the ingestion from our application disk to PostgreSQL. <br>

## For PostgreSQL, make sure you have a viable instance running on your localhost
You have to have a PostgreSQL installed and working - postgresql is open source and
can be downloaded here: https://www.postgresql.org/download/ <br>
Set the user for "postgres" and the password for "admin" <br>
in the scripts_psql we have scripts to create tables, and to insert tables,<br>
they work together with airflow so we can insert data in a pipeline,<br>
otherwise these commands can be used manually too<br>

## To run this apllication first:
Open your terminal and cd into the root of the project, after that run "python app.py" so airflow can be initialized.
Access the airflow webserver in http://localhost:8080/ and login with the username "admin" and password "admin".<br>



## Spark
Spark is what let us process huge files very easily (as long as we split them).
We are using pyspark here and it's managed by the python environment, just make sure to
have Java installed and a $JAVA_HOME path variable set.


## Some of the obtained Results - proof of work
Even though we are showing here a PK for the reviewerID, it wasn't enforced, nor the FK for the fact_reviews was enforced, also, our unique primary key for the fact reviews is a composite key of reviewerID, rproductID (asin) and reviewTime.
![Alt text](./images/ERD_DWH.png?raw=true "Title")
![Alt text](./images/histogramreviewsbyprice.png?raw=true "Title")


### Improvements that could be done
Clean up more the code, <br>
Docker so other people can try it in an easier way, <br>
Ability to incrementally receive more data without deleting everything, etc <br>
